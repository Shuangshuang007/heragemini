// æ‰¹é‡ä» jobs_pipeline åŒæ­¥ jobUrl åˆ° jobs é›†åˆ
require('dotenv').config({ path: '.env.local' });
const { MongoClient } = require('mongodb');
const fs = require('fs').promises;
const path = require('path');

const MONGODB_URI = process.env.MONGODB_URI;
const PIPELINE_DB = 'hera_jobs';
const PIPELINE_COLLECTION = 'jobs_pipeline';
const TARGET_DB = process.env.MONGODB_DB || 'hera';
const TARGET_COLLECTION = process.env.MONGODB_COLLECTION || 'jobs';

// é…ç½®å‚æ•°
const BATCH_SIZE = parseInt(process.env.BATCH_SIZE || '500', 10);
const BULK_WRITE_BATCH = parseInt(process.env.BULK_WRITE_BATCH || '500', 10);
const CONCURRENT_BATCHES = parseInt(process.env.CONCURRENT_BATCHES || '3', 10);
const SLEEP_MS = parseInt(process.env.SLEEP_MS || '100', 10);
const START_AFTER_JOB_ID = process.env.START_AFTER_JOB_ID || null;

// æ¸¸æ ‡æ–‡ä»¶è·¯å¾„
const CURSOR_FILE = path.join(__dirname, 'sync-jobUrl-cursor.json');

// è¯»å–æ¸¸æ ‡ä½ç½®
async function loadCursor() {
  try {
    const data = await fs.readFile(CURSOR_FILE, 'utf8');
    const cursor = JSON.parse(data);
    console.log(`ğŸ“– ä»æ–‡ä»¶è¯»å–æ¸¸æ ‡: ${cursor.lastJobIdentifier || 'null'}`);
    return cursor.lastJobIdentifier || null;
  } catch (error) {
    if (error.code === 'ENOENT') {
      console.log('ğŸ“– æ¸¸æ ‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»å¤´å¼€å§‹');
      return null;
    }
    console.error('âš ï¸  è¯»å–æ¸¸æ ‡æ–‡ä»¶å¤±è´¥:', error.message);
    return null;
  }
}

// ä¿å­˜æ¸¸æ ‡ä½ç½®
async function saveCursor(jobIdentifier, round, totalUpdated, totalSkipped) {
  try {
    const cursor = {
      lastJobIdentifier: jobIdentifier,
      lastRound: round,
      totalUpdated,
      totalSkipped,
      lastUpdated: new Date().toISOString()
    };
    await fs.writeFile(CURSOR_FILE, JSON.stringify(cursor, null, 2), 'utf8');
  } catch (error) {
    console.error('âš ï¸  ä¿å­˜æ¸¸æ ‡æ–‡ä»¶å¤±è´¥:', error.message);
  }
}

async function syncJobUrlBatch() {
  const client = new MongoClient(MONGODB_URI, {
    maxPoolSize: CONCURRENT_BATCHES * 2 + 5
  });
  
  try {
    await client.connect();
    console.log('âœ… Connected to MongoDB\n');
    
    const pipelineCollection = client.db(PIPELINE_DB).collection(PIPELINE_COLLECTION);
    const jobsCollection = client.db(TARGET_DB).collection(TARGET_COLLECTION);
    
    // ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡ä½¿ç”¨ä¿å­˜çš„æ¸¸æ ‡ï¼Œæœ€åä¸º null
    let cursorJobId = START_AFTER_JOB_ID || await loadCursor();
    let round = 1;
    let totalUpdated = 0;
    let totalSkipped = 0;
    const startTime = Date.now();
    
    console.log('ğŸš€ å¼€å§‹æ‰¹é‡åŒæ­¥ jobUrl...\n');
    console.log(`é…ç½®: BATCH_SIZE=${BATCH_SIZE}, BULK_WRITE_BATCH=${BULK_WRITE_BATCH}, CONCURRENT_BATCHES=${CONCURRENT_BATCHES}`);
    console.log(`èµ·å§‹æ¸¸æ ‡: ${cursorJobId || 'ä»å¤´å¼€å§‹'}\n`);
    
    while (true) {
      // 1. ä» pipeline æŸ¥è¯¢ä¸€æ‰¹æœ‰ jobUrl çš„æ–‡æ¡£ï¼ˆä½¿ç”¨ jobIdentifier ç´¢å¼•ï¼‰
      const queryStart = Date.now();
      const query = cursorJobId
        ? {
            jobUrl: { $exists: true, $ne: null, $ne: '' },
            jobIdentifier: { $exists: true, $ne: null, $ne: '', $gt: cursorJobId }
          }
        : {
            jobUrl: { $exists: true, $ne: null, $ne: '' },
            jobIdentifier: { $exists: true, $ne: null, $ne: '' }
          };
      
      const pipelineJobs = await pipelineCollection.find(query, {
        projection: { jobIdentifier: 1, jobUrl: 1 }
      })
      .sort({ jobIdentifier: 1 })
      .limit(BATCH_SIZE)
      .toArray();
      
      const queryTime = Date.now() - queryStart;
      
      if (pipelineJobs.length === 0) {
        console.log(`\nğŸ›‘ ç¬¬ ${round} è½®ååœæ­¢ï¼šæŸ¥è¯¢ä¸åˆ°æ›´å¤šæ•°æ®`);
        // åˆ é™¤æ¸¸æ ‡æ–‡ä»¶ï¼Œè¡¨ç¤ºå·²å®Œæˆ
        try {
          await fs.unlink(CURSOR_FILE);
          console.log('âœ… å·²åˆ é™¤æ¸¸æ ‡æ–‡ä»¶ï¼ˆåŒæ­¥å®Œæˆï¼‰');
        } catch (error) {
          // å¿½ç•¥åˆ é™¤é”™è¯¯
        }
        break;
      }
      
      console.log(`\nğŸ“¦ ç¬¬ ${round} è½®: ä» pipeline æŸ¥è¯¢åˆ° ${pipelineJobs.length} ä¸ªæ–‡æ¡£ (${queryTime}ms)`);
      
      // 2. æ£€æŸ¥ jobs é›†åˆä¸­å“ªäº›éœ€è¦æ›´æ–°ï¼ˆé¿å…é‡å¤ï¼‰
      const checkStart = Date.now();
      const jobIdentifiers = pipelineJobs.map(j => j.jobIdentifier);
      const existingJobs = await jobsCollection.find({
        jobIdentifier: { $in: jobIdentifiers }
      }, {
        projection: { jobIdentifier: 1, jobUrl: 1 }
      }).toArray();
      
      const existingMap = new Map();
      existingJobs.forEach(job => {
        existingMap.set(job.jobIdentifier, job.jobUrl);
      });
      
      // 3. è¿‡æ»¤å‡ºéœ€è¦æ›´æ–°çš„ï¼ˆé¿å…é‡å¤ï¼šåªæ›´æ–°æ²¡æœ‰ jobUrl æˆ– jobUrl ä¸ºç©ºçš„ï¼‰
      const needUpdate = pipelineJobs.filter(pipelineJob => {
        const existingUrl = existingMap.get(pipelineJob.jobIdentifier);
        return !existingUrl || existingUrl === '';
      });
      
      const skipped = pipelineJobs.length - needUpdate.length;
      totalSkipped += skipped;
      const checkTime = Date.now() - checkStart;
      
      console.log(`  æ£€æŸ¥å®Œæˆ: éœ€è¦æ›´æ–° ${needUpdate.length} ä¸ª, è·³è¿‡ ${skipped} ä¸ª (å·²æœ‰ jobUrl) (${checkTime}ms)`);
      
      if (needUpdate.length === 0) {
        console.log(`  â­ï¸  æœ¬è½®æ— éœ€æ›´æ–°ï¼Œç»§ç»­ä¸‹ä¸€è½®...`);
        cursorJobId = pipelineJobs[pipelineJobs.length - 1].jobIdentifier;
        // ä¿å­˜æ¸¸æ ‡ï¼ˆå³ä½¿æ²¡æœ‰æ›´æ–°ä¹Ÿè¦ä¿å­˜ä½ç½®ï¼‰
        await saveCursor(cursorJobId, round, totalUpdated, totalSkipped);
        round++;
        if (SLEEP_MS > 0) {
          await new Promise(resolve => setTimeout(resolve, SLEEP_MS));
        }
        continue;
      }
      
      // 4. æ„å»ºæ‰¹é‡æ›´æ–°æ“ä½œ
      const updateOps = needUpdate.map(pipelineJob => ({
        updateOne: {
          filter: { jobIdentifier: pipelineJob.jobIdentifier },
          update: { $set: { jobUrl: pipelineJob.jobUrl } },
          upsert: false // ä¸åˆ›å»ºæ–°æ–‡æ¡£ï¼Œåªæ›´æ–°å­˜åœ¨çš„
        }
      }));
      
      // 5. åˆ†æ‰¹æ‰§è¡Œ bulkWrite
      const writeStart = Date.now();
      const batches = [];
      for (let i = 0; i < updateOps.length; i += BULK_WRITE_BATCH) {
        batches.push(updateOps.slice(i, i + BULK_WRITE_BATCH));
      }
      
      let roundUpdated = 0;
      for (const batch of batches) {
        const result = await jobsCollection.bulkWrite(batch, { 
          ordered: false,
          writeConcern: { w: 1 } // å¿«é€Ÿç¡®è®¤
        });
        roundUpdated += result.modifiedCount;
      }
      
      const writeTime = Date.now() - writeStart;
      totalUpdated += roundUpdated;
      
      console.log(`  æ‰¹é‡å†™å…¥: ${roundUpdated} ä¸ªæ–‡æ¡£å·²æ›´æ–° (${writeTime}ms)`);
      console.log(`  ç´¯è®¡: å·²æ›´æ–° ${totalUpdated} ä¸ª, è·³è¿‡ ${totalSkipped} ä¸ª`);
      
      // 6. æ›´æ–°æ¸¸æ ‡
      cursorJobId = pipelineJobs[pipelineJobs.length - 1].jobIdentifier;
      
      // 7. ä¿å­˜æ¸¸æ ‡ä½ç½®ï¼ˆæ¯è½®éƒ½ä¿å­˜ï¼Œç¡®ä¿æ–­ç‚¹ç»­ä¼ ï¼‰
      await saveCursor(cursorJobId, round, totalUpdated, totalSkipped);
      
      // 8. è¿›åº¦ç»Ÿè®¡
      const elapsed = Date.now() - startTime;
      const avgPerDoc = roundUpdated > 0 ? (writeTime / roundUpdated).toFixed(2) : 0;
      console.log(`  æœ¬è½®è€—æ—¶: ${queryTime + checkTime + writeTime}ms, å¹³å‡æ¯ä¸ª: ${avgPerDoc}ms`);
      
      round++;
      
      // 9. çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿‡è½½
      if (SLEEP_MS > 0) {
        await new Promise(resolve => setTimeout(resolve, SLEEP_MS));
      }
    }
    
    const totalTime = Date.now() - startTime;
    console.log(`\n\nâœ… æ‰¹é‡åŒæ­¥å®Œæˆï¼`);
    console.log(`  æ€»è½®æ•°: ${round - 1}`);
    console.log(`  æ€»æ›´æ–°: ${totalUpdated} ä¸ªæ–‡æ¡£`);
    console.log(`  æ€»è·³è¿‡: ${totalSkipped} ä¸ªæ–‡æ¡£ï¼ˆå·²æœ‰ jobUrlï¼‰`);
    console.log(`  æ€»è€—æ—¶: ${totalTime}ms (${(totalTime/1000).toFixed(2)}ç§’)`);
    if (totalUpdated > 0) {
      console.log(`  å¹³å‡é€Ÿåº¦: ${(totalUpdated / (totalTime/1000)).toFixed(0)} ä¸ª/ç§’`);
    }
    
  } catch (error) {
    console.error('âŒ Error:', error);
  } finally {
    await client.close();
  }
}

syncJobUrlBatch();


const { MongoClient } = require('mongodb');
const fs = require('fs').promises;
const path = require('path');

const MONGODB_URI = process.env.MONGODB_URI;
const PIPELINE_DB = 'hera_jobs';
const PIPELINE_COLLECTION = 'jobs_pipeline';
const TARGET_DB = process.env.MONGODB_DB || 'hera';
const TARGET_COLLECTION = process.env.MONGODB_COLLECTION || 'jobs';

// é…ç½®å‚æ•°
const BATCH_SIZE = parseInt(process.env.BATCH_SIZE || '500', 10);
const BULK_WRITE_BATCH = parseInt(process.env.BULK_WRITE_BATCH || '500', 10);
const CONCURRENT_BATCHES = parseInt(process.env.CONCURRENT_BATCHES || '3', 10);
const SLEEP_MS = parseInt(process.env.SLEEP_MS || '100', 10);
const START_AFTER_JOB_ID = process.env.START_AFTER_JOB_ID || null;

// æ¸¸æ ‡æ–‡ä»¶è·¯å¾„
const CURSOR_FILE = path.join(__dirname, 'sync-jobUrl-cursor.json');

// è¯»å–æ¸¸æ ‡ä½ç½®
async function loadCursor() {
  try {
    const data = await fs.readFile(CURSOR_FILE, 'utf8');
    const cursor = JSON.parse(data);
    console.log(`ğŸ“– ä»æ–‡ä»¶è¯»å–æ¸¸æ ‡: ${cursor.lastJobIdentifier || 'null'}`);
    return cursor.lastJobIdentifier || null;
  } catch (error) {
    if (error.code === 'ENOENT') {
      console.log('ğŸ“– æ¸¸æ ‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»å¤´å¼€å§‹');
      return null;
    }
    console.error('âš ï¸  è¯»å–æ¸¸æ ‡æ–‡ä»¶å¤±è´¥:', error.message);
    return null;
  }
}

// ä¿å­˜æ¸¸æ ‡ä½ç½®
async function saveCursor(jobIdentifier, round, totalUpdated, totalSkipped) {
  try {
    const cursor = {
      lastJobIdentifier: jobIdentifier,
      lastRound: round,
      totalUpdated,
      totalSkipped,
      lastUpdated: new Date().toISOString()
    };
    await fs.writeFile(CURSOR_FILE, JSON.stringify(cursor, null, 2), 'utf8');
  } catch (error) {
    console.error('âš ï¸  ä¿å­˜æ¸¸æ ‡æ–‡ä»¶å¤±è´¥:', error.message);
  }
}

async function syncJobUrlBatch() {
  const client = new MongoClient(MONGODB_URI, {
    maxPoolSize: CONCURRENT_BATCHES * 2 + 5
  });
  
  try {
    await client.connect();
    console.log('âœ… Connected to MongoDB\n');
    
    const pipelineCollection = client.db(PIPELINE_DB).collection(PIPELINE_COLLECTION);
    const jobsCollection = client.db(TARGET_DB).collection(TARGET_COLLECTION);
    
    // ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡ä½¿ç”¨ä¿å­˜çš„æ¸¸æ ‡ï¼Œæœ€åä¸º null
    let cursorJobId = START_AFTER_JOB_ID || await loadCursor();
    let round = 1;
    let totalUpdated = 0;
    let totalSkipped = 0;
    const startTime = Date.now();
    
    console.log('ğŸš€ å¼€å§‹æ‰¹é‡åŒæ­¥ jobUrl...\n');
    console.log(`é…ç½®: BATCH_SIZE=${BATCH_SIZE}, BULK_WRITE_BATCH=${BULK_WRITE_BATCH}, CONCURRENT_BATCHES=${CONCURRENT_BATCHES}`);
    console.log(`èµ·å§‹æ¸¸æ ‡: ${cursorJobId || 'ä»å¤´å¼€å§‹'}\n`);
    
    while (true) {
      // 1. ä» pipeline æŸ¥è¯¢ä¸€æ‰¹æœ‰ jobUrl çš„æ–‡æ¡£ï¼ˆä½¿ç”¨ jobIdentifier ç´¢å¼•ï¼‰
      const queryStart = Date.now();
      const query = cursorJobId
        ? {
            jobUrl: { $exists: true, $ne: null, $ne: '' },
            jobIdentifier: { $exists: true, $ne: null, $ne: '', $gt: cursorJobId }
          }
        : {
            jobUrl: { $exists: true, $ne: null, $ne: '' },
            jobIdentifier: { $exists: true, $ne: null, $ne: '' }
          };
      
      const pipelineJobs = await pipelineCollection.find(query, {
        projection: { jobIdentifier: 1, jobUrl: 1 }
      })
      .sort({ jobIdentifier: 1 })
      .limit(BATCH_SIZE)
      .toArray();
      
      const queryTime = Date.now() - queryStart;
      
      if (pipelineJobs.length === 0) {
        console.log(`\nğŸ›‘ ç¬¬ ${round} è½®ååœæ­¢ï¼šæŸ¥è¯¢ä¸åˆ°æ›´å¤šæ•°æ®`);
        // åˆ é™¤æ¸¸æ ‡æ–‡ä»¶ï¼Œè¡¨ç¤ºå·²å®Œæˆ
        try {
          await fs.unlink(CURSOR_FILE);
          console.log('âœ… å·²åˆ é™¤æ¸¸æ ‡æ–‡ä»¶ï¼ˆåŒæ­¥å®Œæˆï¼‰');
        } catch (error) {
          // å¿½ç•¥åˆ é™¤é”™è¯¯
        }
        break;
      }
      
      console.log(`\nğŸ“¦ ç¬¬ ${round} è½®: ä» pipeline æŸ¥è¯¢åˆ° ${pipelineJobs.length} ä¸ªæ–‡æ¡£ (${queryTime}ms)`);
      
      // 2. æ£€æŸ¥ jobs é›†åˆä¸­å“ªäº›éœ€è¦æ›´æ–°ï¼ˆé¿å…é‡å¤ï¼‰
      const checkStart = Date.now();
      const jobIdentifiers = pipelineJobs.map(j => j.jobIdentifier);
      const existingJobs = await jobsCollection.find({
        jobIdentifier: { $in: jobIdentifiers }
      }, {
        projection: { jobIdentifier: 1, jobUrl: 1 }
      }).toArray();
      
      const existingMap = new Map();
      existingJobs.forEach(job => {
        existingMap.set(job.jobIdentifier, job.jobUrl);
      });
      
      // 3. è¿‡æ»¤å‡ºéœ€è¦æ›´æ–°çš„ï¼ˆé¿å…é‡å¤ï¼šåªæ›´æ–°æ²¡æœ‰ jobUrl æˆ– jobUrl ä¸ºç©ºçš„ï¼‰
      const needUpdate = pipelineJobs.filter(pipelineJob => {
        const existingUrl = existingMap.get(pipelineJob.jobIdentifier);
        return !existingUrl || existingUrl === '';
      });
      
      const skipped = pipelineJobs.length - needUpdate.length;
      totalSkipped += skipped;
      const checkTime = Date.now() - checkStart;
      
      console.log(`  æ£€æŸ¥å®Œæˆ: éœ€è¦æ›´æ–° ${needUpdate.length} ä¸ª, è·³è¿‡ ${skipped} ä¸ª (å·²æœ‰ jobUrl) (${checkTime}ms)`);
      
      if (needUpdate.length === 0) {
        console.log(`  â­ï¸  æœ¬è½®æ— éœ€æ›´æ–°ï¼Œç»§ç»­ä¸‹ä¸€è½®...`);
        cursorJobId = pipelineJobs[pipelineJobs.length - 1].jobIdentifier;
        // ä¿å­˜æ¸¸æ ‡ï¼ˆå³ä½¿æ²¡æœ‰æ›´æ–°ä¹Ÿè¦ä¿å­˜ä½ç½®ï¼‰
        await saveCursor(cursorJobId, round, totalUpdated, totalSkipped);
        round++;
        if (SLEEP_MS > 0) {
          await new Promise(resolve => setTimeout(resolve, SLEEP_MS));
        }
        continue;
      }
      
      // 4. æ„å»ºæ‰¹é‡æ›´æ–°æ“ä½œ
      const updateOps = needUpdate.map(pipelineJob => ({
        updateOne: {
          filter: { jobIdentifier: pipelineJob.jobIdentifier },
          update: { $set: { jobUrl: pipelineJob.jobUrl } },
          upsert: false // ä¸åˆ›å»ºæ–°æ–‡æ¡£ï¼Œåªæ›´æ–°å­˜åœ¨çš„
        }
      }));
      
      // 5. åˆ†æ‰¹æ‰§è¡Œ bulkWrite
      const writeStart = Date.now();
      const batches = [];
      for (let i = 0; i < updateOps.length; i += BULK_WRITE_BATCH) {
        batches.push(updateOps.slice(i, i + BULK_WRITE_BATCH));
      }
      
      let roundUpdated = 0;
      for (const batch of batches) {
        const result = await jobsCollection.bulkWrite(batch, { 
          ordered: false,
          writeConcern: { w: 1 } // å¿«é€Ÿç¡®è®¤
        });
        roundUpdated += result.modifiedCount;
      }
      
      const writeTime = Date.now() - writeStart;
      totalUpdated += roundUpdated;
      
      console.log(`  æ‰¹é‡å†™å…¥: ${roundUpdated} ä¸ªæ–‡æ¡£å·²æ›´æ–° (${writeTime}ms)`);
      console.log(`  ç´¯è®¡: å·²æ›´æ–° ${totalUpdated} ä¸ª, è·³è¿‡ ${totalSkipped} ä¸ª`);
      
      // 6. æ›´æ–°æ¸¸æ ‡
      cursorJobId = pipelineJobs[pipelineJobs.length - 1].jobIdentifier;
      
      // 7. ä¿å­˜æ¸¸æ ‡ä½ç½®ï¼ˆæ¯è½®éƒ½ä¿å­˜ï¼Œç¡®ä¿æ–­ç‚¹ç»­ä¼ ï¼‰
      await saveCursor(cursorJobId, round, totalUpdated, totalSkipped);
      
      // 8. è¿›åº¦ç»Ÿè®¡
      const elapsed = Date.now() - startTime;
      const avgPerDoc = roundUpdated > 0 ? (writeTime / roundUpdated).toFixed(2) : 0;
      console.log(`  æœ¬è½®è€—æ—¶: ${queryTime + checkTime + writeTime}ms, å¹³å‡æ¯ä¸ª: ${avgPerDoc}ms`);
      
      round++;
      
      // 9. çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿‡è½½
      if (SLEEP_MS > 0) {
        await new Promise(resolve => setTimeout(resolve, SLEEP_MS));
      }
    }
    
    const totalTime = Date.now() - startTime;
    console.log(`\n\nâœ… æ‰¹é‡åŒæ­¥å®Œæˆï¼`);
    console.log(`  æ€»è½®æ•°: ${round - 1}`);
    console.log(`  æ€»æ›´æ–°: ${totalUpdated} ä¸ªæ–‡æ¡£`);
    console.log(`  æ€»è·³è¿‡: ${totalSkipped} ä¸ªæ–‡æ¡£ï¼ˆå·²æœ‰ jobUrlï¼‰`);
    console.log(`  æ€»è€—æ—¶: ${totalTime}ms (${(totalTime/1000).toFixed(2)}ç§’)`);
    if (totalUpdated > 0) {
      console.log(`  å¹³å‡é€Ÿåº¦: ${(totalUpdated / (totalTime/1000)).toFixed(0)} ä¸ª/ç§’`);
    }
    
  } catch (error) {
    console.error('âŒ Error:', error);
  } finally {
    await client.close();
  }
}

syncJobUrlBatch();













